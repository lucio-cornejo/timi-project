{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43842aaa",
   "metadata": {},
   "source": [
    "# Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d4bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import (\n",
    "  COLUMNA_OBJETIVO,\n",
    "  PREDICTORES_NUMERICOS,\n",
    "  PREDICTORES_CATEGORICOS\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33d009",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "datos = (pd\n",
    "  .read_csv('census_income/census-income.csv')\n",
    "  # Excluir las columnas 'key' e 'instance weight'\n",
    "  [[*PREDICTORES_NUMERICOS, *PREDICTORES_CATEGORICOS, COLUMNA_OBJETIVO]]\n",
    ")\n",
    "\n",
    "# Corregir los tipos de variables\n",
    "for pred_num in PREDICTORES_NUMERICOS:\n",
    "  datos[pred_num] = pd.to_numeric(datos[pred_num], errors = 'raise')\n",
    "\n",
    "for pred_cat in PREDICTORES_CATEGORICOS:\n",
    "  datos[pred_cat] = pd.Categorical(datos[pred_cat])\n",
    "\n",
    "datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d790b15e",
   "metadata": {},
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251ee84",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Definimos una función que calcule las métricas relevantes\n",
    "# asociadas a la matriz de confusión, dado el par de parámetros\n",
    "# datos_observados y datos_predichos\n",
    "def mostrar_metricas_relevantes_del_modelo(y_observado, y_predicho) -> None:\n",
    "  \"\"\"\n",
    "  Suponemos que ambos parámetros contienen solo elementos 1 y 0,\n",
    "  donde 1 representa el caso exitoso. Es decir, para este proyecto,\n",
    "  1 representa cuando la persona debe pagar impuesto.\n",
    "  \"\"\"\n",
    "  tp, fn, fp, tn = confusion_matrix(y_observado, y_predicho, labels = [1, 0]).ravel()\n",
    "  exactitud = (tp + tn) / (tp + fp + tn + fn)\n",
    "  sensibilidad = tp / (tp + fn)\n",
    "  especificidad = tn / (tn + fp)\n",
    "\n",
    "  print('\\nCantidades en la variable por predecir:', y_observado.value_counts())\n",
    "\n",
    "  print('\\nMatriz de confusión:\\n', confusion_matrix(y_observado, y_predicho, labels = [1, 0]))\n",
    "  print(f'\\nExactitud: {exactitud}')\n",
    "  print(f'\\nSensibilidad: {sensibilidad}')\n",
    "  print(f'\\nEspecificidad: {especificidad}')\n",
    "\n",
    "  # Coeficiente de Kappa de Cohen\n",
    "  print(f'\\nCoeficiente de Kappa de Cohen: {cohen_kappa_score(y_observado, y_predicho)}')\n",
    "\n",
    "  return;\n",
    "\n",
    "# Definimos una función para calcular la curva ROC y el AUC,\n",
    "# dado un modelo, ya entrenado y capaz de predecir probabilidad\n",
    "# de pertenencia a una clase; además de un cojunto de test\n",
    "def graficar_curva_roc_con_auc(\n",
    "  nombre_modelo, modelo, predictores_test, target_test\n",
    ") -> None:\n",
    "  y_pred_prob = modelo.predict_proba(predictores_test)[:, 1]\n",
    "\n",
    "  # Calcular curva ROC y su AUC\n",
    "  fpr, tpr, _ = roc_curve(target_test, y_pred_prob)\n",
    "  auc_score = roc_auc_score(target_test, y_pred_prob)\n",
    "\n",
    "  plt.figure(figsize = (8, 6))\n",
    "  plt.plot(fpr, tpr, label = f\"Curva ROC (AUC = {auc_score:.3f})\")\n",
    "  plt.plot([0, 1], [0, 1], 'k--', label = 'Clasificación aleatoria')\n",
    "  plt.xlim([0.0, 1.0])\n",
    "  plt.ylim([0.0, 1.05])\n",
    "  plt.xlabel(\"1 - Especificidad\")\n",
    "  plt.ylabel(\"Sensibilidad\")\n",
    "  plt.title(f\"Curva ROC del modelo {nombre_modelo}\")\n",
    "  plt.legend(loc = \"lower right\")\n",
    "  plt.show()\n",
    "\n",
    "  return;\n",
    "\n",
    "# Definimos una función para calcular el punto de corte óptimo,\n",
    "# para el mejor equilibrio posible entre sensibilidad y especificidad,\n",
    "# según el criterio del índice J de Youden\n",
    "def optimizar_punto_de_corte_segun_criterio_de_youden(\n",
    "  modelo, predictores_test, target_test\n",
    ") -> float:\n",
    "  y_pred_prob = modelo.predict_proba(predictores_test)[:, 1]\n",
    "  fpr, tpr, thresholds = roc_curve(target_test, y_pred_prob)\n",
    "\n",
    "  df_youden = pd.DataFrame({'thresholds' : thresholds, 'J': tpr - fpr})\n",
    "  punto_de_corte_optimo = df_youden[df_youden[\"J\"] == df_youden[\"J\"].max()][\"thresholds\"].values[0]\n",
    "  return punto_de_corte_optimo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4ca211",
   "metadata": {},
   "source": [
    "## Sobre la partición en entrenamiento y test\n",
    "\n",
    "No es recomendable comparar modelos en base a un mismo conjunto\n",
    "de test. Esto porque sino podría generarse overfitting respecto\n",
    "al conjunto de test, perdiendo así generalizabilidad de los modelos.\n",
    "Aquello en el sentido que podria producirse un 'sesgo de confirmación',\n",
    "pues sería posible que, tras emplear el mismo conjunto de test para\n",
    "todos los modelos, no se obtenga el modelo que se adecúe mejor\n",
    "a otros datos nuevos, sino solo a esa muestra en particular.\n",
    "\n",
    "En ese sentido, para cada modelo se realizará una partición\n",
    "diferente en entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b8850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijamos diferentes semillas para la partición train/test\n",
    "# en cada modelo\n",
    "RANDOM_STATE_REG = 42\n",
    "RANDOM_STATE_BOSQUE = 420\n",
    "RANDOM_STATE_XGB = 6174\n",
    "\n",
    "X = pd.get_dummies(\n",
    "  datos.drop(columns = [COLUMNA_OBJETIVO]),\n",
    "  columns = PREDICTORES_CATEGORICOS,\n",
    "  # Descartamos una columna del total de columnas creadas por variable\n",
    "  # categórica, para intentar evitar multicolinearidad entre los predictores\n",
    "  drop_first = True,\n",
    "  dtype = 'int64'\n",
    ")\n",
    "\n",
    "y = datos[COLUMNA_OBJETIVO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7737f",
   "metadata": {},
   "source": [
    "## Regresión logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac985b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca1e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "  X, y, test_size = 0.2, random_state = RANDOM_STATE_REG\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8c68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estandarizamos los predictores\n",
    "estandarizador = StandardScaler()\n",
    "X_train_std = estandarizador.fit_transform(X_train_reg)\n",
    "X_test_std = estandarizador.transform(X_test_reg)\n",
    "\n",
    "modelo_reg = LogisticRegression(max_iter = 1000, random_state = RANDOM_STATE_REG)\n",
    "modelo_reg.fit(X_train_std, y_train_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99da30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones del modelo, con punto de corte c = 0.5\n",
    "y_pred_reg = modelo_reg.predict(X_test_std)\n",
    "mostrar_metricas_relevantes_del_modelo(y_test_reg, y_pred_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cbcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_curva_roc_con_auc(\n",
    "  nombre_modelo = 'regresión logística',\n",
    "  modelo = modelo_reg,\n",
    "  predictores_test = X_test_std,\n",
    "  target_test = y_test_reg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b20c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular el punto de corte para el mejor equilibrio posible\n",
    "# entre sensibilidad y especificidad\n",
    "punto_de_corte_optimo_reg = optimizar_punto_de_corte_segun_criterio_de_youden(\n",
    "  modelo_reg, X_test_std, y_test_reg\n",
    ")\n",
    "print(f'Punto de corte óptimo: {punto_de_corte_optimo_reg}')\n",
    "\n",
    "y_pred_prob_reg = modelo_reg.predict_proba(X_test_std)[:, 1]\n",
    "y_pred_opt_reg = (y_pred_prob_reg >= punto_de_corte_optimo_reg) + 0\n",
    "\n",
    "mostrar_metricas_relevantes_del_modelo(y_test_reg, y_pred_opt_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8fd70d",
   "metadata": {},
   "source": [
    "### Importancia de los predictores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64fc10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_predictores_reg = pd.DataFrame({\n",
    "  'Predictor': X.columns,\n",
    "  # Coeficientes estandarizados\n",
    "  'Coeficiente': modelo_reg.coef_[0]\n",
    "})\n",
    "importancia_predictores_reg = (importancia_predictores_reg\n",
    "  .assign(abs_coef = lambda d: d['Coeficiente'].abs())\n",
    "  .sort_values(by = 'abs_coef', ascending = False)\n",
    "  .loc[:, ['Predictor', 'Coeficiente']]\n",
    ")\n",
    "\n",
    "importancia_predictores_reg.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2e04b",
   "metadata": {},
   "source": [
    "**Interpretación**: \n",
    "\"\"\"\n",
    "NO OLVIDAR QUE LOS BETA_I SE INTERPRETAN EN BASE Al CAMBIO DEL \n",
    "PREDICTOR X_I EN DESVIACION_ESTANDAR(X_I), NO CAMBIO EN 1 NOMÁS.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79e422",
   "metadata": {},
   "source": [
    "## Bosque aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df5a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3741db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bosque, X_test_bosque, y_train_bosque, y_test_bosque = train_test_split(\n",
    "  X, y, test_size = 0.2, random_state = RANDOM_STATE_BOSQUE\n",
    ")\n",
    "\n",
    "modelo_bosque = RandomForestClassifier(n_estimators = 100, random_state = RANDOM_STATE_BOSQUE)\n",
    "modelo_bosque.fit(X_train_bosque, y_train_bosque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_bosque = modelo_bosque.predict(X_test_bosque)\n",
    "mostrar_metricas_relevantes_del_modelo(y_test_bosque, y_pred_bosque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ed507",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_curva_roc_con_auc(\n",
    "  nombre_modelo = 'bosque aleatorio',\n",
    "  modelo = modelo_bosque,\n",
    "  predictores_test = X_test_bosque,\n",
    "  target_test = y_test_bosque\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b43acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "punto_de_corte_optimo_bosque = optimizar_punto_de_corte_segun_criterio_de_youden(\n",
    "  modelo_bosque, X_test_bosque, y_test_bosque\n",
    ")\n",
    "print(f'Punto de corte óptimo: {punto_de_corte_optimo_bosque}')\n",
    "\n",
    "y_pred_prob_bosque = modelo_bosque.predict_proba(X_test_bosque)[:, 1]\n",
    "y_pred_opt_bosque = (y_pred_prob_bosque >= punto_de_corte_optimo_bosque) + 0\n",
    "\n",
    "mostrar_metricas_relevantes_del_modelo(y_test_bosque, y_pred_opt_bosque)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f77c8f3",
   "metadata": {},
   "source": [
    "### Importancia de los predictores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72726b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_predictores_bosque = (pd.DataFrame({\n",
    "  'Predictor' : X.columns,\n",
    "  'Importancia': modelo_bosque.feature_importances_\n",
    "})\n",
    "  .sort_values('Importancia', ascending = True)\n",
    "  # Seleccionar solo los 20 predictores más importantes\n",
    "  .tail(20)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 10))\n",
    "ax.barh(\n",
    "  importancia_predictores_bosque['Predictor'].values,\n",
    "  importancia_predictores_bosque['Importancia'].values\n",
    ")\n",
    "plt.title(\"Importancia de los predictores en el modelo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b50c7ab",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8997141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "\n",
    "X_xgb = X.copy()\n",
    "\n",
    "# Remover los caracteres en los nombres de las columnas que generan\n",
    "# error al entrenar el modelo\n",
    "# Fuente: https://stackoverflow.com/a/50633571\n",
    "regex = re.compile(r\"\\[|\\]|<\", re.IGNORECASE)\n",
    "X_xgb.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_xgb.columns.values]\n",
    "\n",
    "X_train_xgb, X_test_xgb, y_train_xgb, y_test_xgb = train_test_split(\n",
    "  X_xgb, y, test_size = 0.2, random_state = RANDOM_STATE_XGB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b4fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { 'alpha': [0.01, 0.1, 1, 10] }\n",
    "\n",
    "cv = KFold(n_splits = 5, shuffle = True, random_state = RANDOM_STATE_XGB)\n",
    "\n",
    "proporcion_negativos_positivos = (y.shape[0] - y.sum()) / y.sum()\n",
    "\n",
    "modelo_cv_xgb = xgb.XGBClassifier(\n",
    "  # Calculamos la raíz cuadra de la proporción de negativos\n",
    "  # y positivos, pues la cantidad de casos positivos es tan pequeña\n",
    "  scale_pos_weight = np.sqrt(proporcion_negativos_positivos),\n",
    "  use_label_encoder = False,\n",
    "  random_state = RANDOM_STATE_XGB\n",
    ")\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "  estimator = modelo_cv_xgb, \n",
    "  param_grid = param_grid, \n",
    "  cv = cv, \n",
    "  scoring = kappa_scorer, \n",
    "  n_jobs = -1,\n",
    "  verbose = 1\n",
    ")\n",
    "grid_search.fit(X_train_xgb, y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c263bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mejor_xgb = grid_search.best_estimator_\n",
    "mejor_xgb.fit(X_train_xgb, y_train_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b7b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = mejor_xgb.predict(X_test_xgb)\n",
    "mostrar_metricas_relevantes_del_modelo(y_test_xgb, y_pred_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "graficar_curva_roc_con_auc(\n",
    "  nombre_modelo = 'XGB',\n",
    "  modelo = mejor_xgb,\n",
    "  predictores_test = X_test_xgb,\n",
    "  target_test = y_test_xgb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e43e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "punto_de_corte_optimo_xgb = optimizar_punto_de_corte_segun_criterio_de_youden(\n",
    "  mejor_xgb, X_test_xgb, y_test_xgb\n",
    ")\n",
    "print(f'Punto de corte óptimo: {punto_de_corte_optimo_xgb}')\n",
    "\n",
    "y_pred_prob_xgb = mejor_xgb.predict_proba(X_test_xgb)[:, 1]\n",
    "y_pred_opt_xgb = (y_pred_prob_xgb >= punto_de_corte_optimo_xgb) + 0\n",
    "\n",
    "mostrar_metricas_relevantes_del_modelo(y_test_xgb, y_pred_opt_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c38dc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "importancia_predictores_xgb = (pd.DataFrame({\n",
    "  # No es necesario extraer las columns de X_xgb, pues tienen\n",
    "  # el mismo orden que en X. Emplearemos las columnas de X \n",
    "  # por consistencia con los modelos previos.\n",
    "  'Predictor' : X.columns,\n",
    "  'Importancia': mejor_xgb.feature_importances_\n",
    "})\n",
    "  .sort_values('Importancia', ascending = True)\n",
    "  # Seleccionar solo los 20 predictores más importantes\n",
    "  .tail(20)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (6, 10))\n",
    "ax.barh(\n",
    "  importancia_predictores_xgb['Predictor'].values,\n",
    "  importancia_predictores_xgb['Importancia'].values\n",
    ")\n",
    "plt.title(\"Importancia de los predictores en el modelo\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
